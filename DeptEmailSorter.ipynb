{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqFi_MZAPZyp"
      },
      "source": [
        "# **ARTI 502 â€“ Deep Learning Project**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpGxQRxuQXyf"
      },
      "source": [
        "\n",
        "\n",
        "-------\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNBh_eqvAnuD"
      },
      "source": [
        "# **Step 1: Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3eOaYthIE_C"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import re\n",
        "from nltk import pos_tag\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense, GRU\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Dropout\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from gensim.models import Word2Vec\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "#tf.random.set_seed(42)\n",
        "#np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gTIUxcfYILqr"
      },
      "outputs": [],
      "source": [
        "# Load your dataset\n",
        "df = pd.read_csv('emails.csv')\n",
        "df['Email'] = df['Email'].astype(str)  # Ensure all emails are strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14v1yfhtIZAQ"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1hOMRfcHRJO"
      },
      "source": [
        "# **Step 2: Exploratory Data Analysis (EDA)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFJCjHlwHzT6"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5dodmvWH2Z7"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfptBsyeH4nn"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeM-iy8mH5TO"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usFot9zGHZDh"
      },
      "outputs": [],
      "source": [
        "print(df['Department'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GJE3Gz_HsEg"
      },
      "outputs": [],
      "source": [
        "# Distribution of Department\n",
        "department_distribution = df['Department'].value_counts()\n",
        "department_distribution.plot(kind='bar', title='Distribution of Department')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLwh_8OTHv3G"
      },
      "outputs": [],
      "source": [
        "# Word Count Distribution\n",
        "df['WordCount'] = df['Email'].apply(lambda x: len(word_tokenize(str(x))))\n",
        "word_count_distribution = df.groupby('Department')['WordCount'].plot(kind='hist', alpha=0.5, legend=True)\n",
        "plt.title('Word Count Distribution by Department')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkCSSrLO-w1n"
      },
      "source": [
        "# **Step 3: Data Preprocessing using NLP**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ctqc9aVK_Dyh"
      },
      "source": [
        "\n",
        "* Conversion of POS Tags\n",
        "* Removing Greetings\n",
        "* Removing Special Characters\n",
        "* Removing Common Email Signatures\n",
        "* Converting to Lowercase\n",
        "* Tokenization\n",
        "* Part-of-Speech Tagging\n",
        "* Lemmatization and Stemming\n",
        "* Removing Stopwords\n",
        "* Removing Specific Words\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ATHHDje7IdCR"
      },
      "outputs": [],
      "source": [
        "# Function to convert NLTK's pos tags to the format recognized by WordNetLemmatizer\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return 'a'  # Adjective\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return 'v'  # Verb\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return 'n'  # Noun\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return 'r'  # Adverb\n",
        "    else:\n",
        "        return 'n'  # Default to noun if not found\n",
        "\n",
        "# Function to clean and process email text\n",
        "def clean_and_process_email(email_text):\n",
        "    # Define a regular expression pattern for common greetings with names\n",
        "    greeting_pattern = re.compile(r'\\b(?:Hi|Hello|Dear|Greetings|Good morning|Good evening|Good day|I hope this email finds you well|I trust this email finds you well|I hope you are doing well)\\b\\s+(\\w+\\s+)(\\w+\\s+)?(\\w+)', flags=re.IGNORECASE)\n",
        "    processed_email = re.sub(greeting_pattern, '', email_text)\n",
        "\n",
        "    # Remove special characters\n",
        "    processed_email = re.sub(r'[^a-zA-Z\\s]', '', processed_email)\n",
        "\n",
        "    # Remove common email signatures\n",
        "    signature_pattern = re.compile(r'\\b(thanks|thank you|regards|cheers|sincerely|best regards|kind regards|warm regards|with regards)\\b.*', flags=re.IGNORECASE)\n",
        "    processed_email = re.sub(signature_pattern, '', processed_email)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    processed_email = processed_email.lower()\n",
        "\n",
        "    # Tokenize the email\n",
        "    tokens = word_tokenize(processed_email)\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "    # Initialize lemmatizer and stemmer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    # Remove stopwords, lemmatize, and stem the tokens\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    preprocessed_tokens = [stemmer.stem(lemmatizer.lemmatize(token, get_wordnet_pos(tag))) for token, tag in tagged_tokens if token not in stop_words]\n",
        "\n",
        "    # Additional removal of specific words after lemmatization\n",
        "    preprocessed_tokens = [token for token in preprocessed_tokens if token not in ['email', 'finds', 'well']]\n",
        "\n",
        "    # Return the preprocessed tokens\n",
        "    return ' '.join(preprocessed_tokens)  # Join tokens into a single string for the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-m5mYVQtIqzj"
      },
      "outputs": [],
      "source": [
        "df['Processed_Email'] = df['Email'].apply(clean_and_process_email)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSr84lsqA0dy"
      },
      "source": [
        "\n",
        "\n",
        "* Word2Vec Word Embedding\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xyledZ9I6DB",
        "outputId": "1eb741a9-ec4b-4f54-e4d2-daa37eab27a2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df['Processed_Email'])\n",
        "vocab_size = len(tokenizer.word_index)+1\n",
        "\n",
        "# Convert the texts to sequences\n",
        "sequences = tokenizer.texts_to_sequences(df['Processed_Email'])\n",
        "\n",
        "# Pad the sequences to have the same length\n",
        "maxlen = max(len(seq) for seq in sequences)  # You can set a fixed maxlen if you prefer\n",
        "padded_sequences = pad_sequences(sequences, maxlen=158, padding='post', truncating='post')\n",
        "# padded_sequences [8]\n",
        "word2vec_model = Word2Vec(sentences=df['Processed_Email'], vector_size=30, window=3, min_count=2, workers=4, seed = 25)\n",
        "word2vec_model.save(\"word2vec.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ortX7ZWHI_5H"
      },
      "outputs": [],
      "source": [
        "def target(x):\n",
        "  if x =='HR':\n",
        "    return 0\n",
        "  elif x == 'IT':\n",
        "    return 1\n",
        "  elif x == 'Customer Service':\n",
        "    return 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "c22-ImziJR8o"
      },
      "outputs": [],
      "source": [
        "df['Department'] = df['Department'].map(target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YS61e3zDJ_Cg"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df['Processed_Email'])\n",
        "sequences = tokenizer.texts_to_sequences(df['Processed_Email'])\n",
        "word_index = tokenizer.word_index\n",
        "max_sequence_length = max(len(seq) for seq in sequences)\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "embedding_dim = 30\n",
        "vocab_size = len(word_index) + 1\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < vocab_size:\n",
        "        try:\n",
        "            embedding_vector = word2vec_model.wv[word]\n",
        "            if embedding_vector is not None:\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "        except KeyError:\n",
        "            pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDurddO6BShy"
      },
      "source": [
        "# **Step 4: Model Development and Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTZPi1eWBdIt"
      },
      "source": [
        "> ## **BiGRU Model**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAEsqbe-Jytn"
      },
      "outputs": [],
      "source": [
        "num_classes = 3\n",
        "\n",
        "#Define and build the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_sequence_length, trainable=True))\n",
        "model.add(Bidirectional(GRU(64, return_sequences=True)))\n",
        "model.add(Bidirectional(GRU(128, return_sequences=True)))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Bidirectional(GRU(64, return_sequences=False)))\n",
        "model.add(Dense(num_classes, activation = 'softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True, mode='min')\n",
        "\n",
        "# Splitting the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, df['Department'], test_size=0.2, random_state=42)\n",
        "\n",
        "#Training the model\n",
        "history = model.fit(X_train, y_train, batch_size=16, epochs=40, validation_split=0.1,callbacks=[early_stopping]) # 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuxSFd60CxO3"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUfFSyW-KglV"
      },
      "source": [
        "\n",
        "\n",
        "> ## **BiLSTM Model**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r28FHoEkK6xs"
      },
      "outputs": [],
      "source": [
        "num_classes = 3\n",
        "\n",
        "#Define and build the model\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_sequence_length, trainable=True))\n",
        "model2.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
        "model2.add(Dropout(0.3))\n",
        "model2.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "model2.add(Bidirectional(LSTM(32, return_sequences=False)))\n",
        "model2.add(Dense(num_classes, activation = 'softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True, mode='min')\n",
        "\n",
        "# Splitting the dataset\n",
        "X2_train, X2_test, y2_train, y2_test = train_test_split(padded_sequences, df['Department'], test_size=0.2, random_state=42)\n",
        "\n",
        "#Training the model\n",
        "history2 = model2.fit(X2_train, y2_train, batch_size=16, epochs=100, validation_split=0.1,callbacks=[early_stopping]) # 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXHLdQM2K9aU"
      },
      "outputs": [],
      "source": [
        "model2.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS-HBfbSD48K"
      },
      "source": [
        "# **Step 5: Model Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG3HAfoMJhLq"
      },
      "source": [
        "> ## **BiGRU Model**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPq5ei1p7vEv"
      },
      "outputs": [],
      "source": [
        "# Evaluating the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n",
        "\n",
        "# Predicting and calculating metrics\n",
        "y_pred = model.predict(X_test)\n",
        "recall = recall_score(y_test, np.argmax(y_pred, axis=1), average='weighted')\n",
        "precision = precision_score(y_test, np.argmax(y_pred, axis=1), average='weighted')\n",
        "f1 = f1_score(y_test, np.argmax(y_pred, axis=1), average='weighted')\n",
        "\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"F1-Score: {f1}\")\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, np.argmax(y_pred, axis=1))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SadXnrjG3I1"
      },
      "outputs": [],
      "source": [
        "# Visualize training and validation accuracy\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBnHZpW3Esvk"
      },
      "outputs": [],
      "source": [
        "# Visualize Confusion Matrix\n",
        "plt.figure(figsize=(7, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='g', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xcc3mkXE2UT"
      },
      "outputs": [],
      "source": [
        "# Visualize Model Performance Metrics\n",
        "metrics = {\n",
        "    'Accuracy': accuracy,\n",
        "    'Precision': precision,\n",
        "    'Recall': recall,\n",
        "    'F1-Score': f1\n",
        "}\n",
        "metric_names = list(metrics.keys())\n",
        "metric_values = list(metrics.values())\n",
        "plt.figure(figsize=(7, 6))\n",
        "plt.bar(metric_names, metric_values, color=['blue', 'green', 'orange', 'cyan'])\n",
        "plt.title('Model Performance Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT9A7D1OMAGp"
      },
      "source": [
        "> ## **BiLSTM Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4clNpsNMIzY"
      },
      "outputs": [],
      "source": [
        "# Evaluating the model\n",
        "loss2, accuracy2 = model2.evaluate(X2_test, y2_test)\n",
        "print(f\"Test Loss: {loss2}, Test Accuracy: {accuracy2}\")\n",
        "\n",
        "# Predicting and calculating metrics\n",
        "y2_pred = model2.predict(X2_test)\n",
        "recall2 = recall_score(y2_test, np.argmax(y2_pred, axis=1), average='weighted')\n",
        "precision2 = precision_score(y2_test, np.argmax(y2_pred, axis=1), average='weighted')\n",
        "f12 = f1_score(y2_test, np.argmax(y2_pred, axis=1), average='weighted')\n",
        "\n",
        "print(f\"Recall: {recall2}\")\n",
        "print(f\"Precision: {precision2}\")\n",
        "print(f\"F1-Score: {f12}\")\n",
        "\n",
        "# Confusion matrix\n",
        "cm2 = confusion_matrix(y2_test, np.argmax(y2_pred, axis=1))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyWDbQ5hODZw"
      },
      "outputs": [],
      "source": [
        "# Visualize training and validation accuracy\n",
        "plt.plot(history2.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history2.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCCGG5LlOJe5"
      },
      "outputs": [],
      "source": [
        "# Visualize Confusion Matrix\n",
        "plt.figure(figsize=(7, 6))\n",
        "sns.heatmap(cm2, annot=True, fmt='g', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHGssq5ZOXIG"
      },
      "outputs": [],
      "source": [
        "# Visualize Model Performance Metrics\n",
        "metrics = {\n",
        "    'Accuracy': accuracy2,\n",
        "    'Precision': precision2,\n",
        "    'Recall': recall2,\n",
        "    'F1-Score': f12\n",
        "}\n",
        "metric_names = list(metrics.keys())\n",
        "metric_values = list(metrics.values())\n",
        "plt.figure(figsize=(7, 6))\n",
        "plt.bar(metric_names, metric_values, color=['blue', 'green', 'orange', 'cyan'])\n",
        "plt.title('Model Performance Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "84a04107e86851c28f6d4c398d5a5a2b1f757414cc48781cbe0e3ea069fb0f97"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
